# Real-Time ML Model Serving API

![Go](https://img.shields.io/badge/Go-1.21%2B-00ADD8)
![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Redis](https://img.shields.io/badge/Redis-Cache-red)
![License](https://img.shields.io/badge/License-MIT-green)
![Performance](https://img.shields.io/badge/Latency-%3C1ms-brightgreen)

[English](#english) | [PortuguÃªs](#portuguÃªs)

---

<a name="english"></a>
## ğŸ‡¬ğŸ‡§ English

### ğŸ“Š Overview

**Real-Time ML Model Serving API** is a high-performance, production-ready API built with **Go** for serving machine learning models in real-time. It features intelligent caching with Redis, model drift monitoring, A/B testing, prediction logging, auto-scaling capabilities, and seamless integration with MLflow for model management.

This project demonstrates best practices for deploying ML models in production with sub-millisecond latency and high throughput.

### âœ¨ Key Features

- **High-Performance Go Server**
  - Sub-millisecond response times
  - Concurrent request handling
  - Efficient memory management
  - RESTful API design

- **Intelligent Caching**
  - Redis integration for prediction caching
  - Cache invalidation strategies
  - TTL-based expiration
  - Cache hit rate monitoring

- **MLOps Capabilities**
  - Model versioning
  - A/B testing framework
  - Drift detection and monitoring
  - Prediction logging for retraining
  - MLflow integration

- **Production-Ready**
  - Docker containerization
  - Kubernetes deployment configs
  - Health checks and metrics
  - Graceful shutdown
  - Auto-scaling support

### ğŸ—ï¸ Architecture

```
realtime-ml-serving-api/
â”œâ”€â”€ server/                 # Go server
â”‚   â”œâ”€â”€ main.go
â”‚   â”œâ”€â”€ handlers/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ middleware/
â”œâ”€â”€ client/                 # Python client
â”‚   â”œâ”€â”€ ml_client.py
â”‚   â””â”€â”€ model_trainer.py
â”œâ”€â”€ models/                 # Trained models
â”œâ”€â”€ config/                 # Configuration
â”œâ”€â”€ tests/                  # Tests
â””â”€â”€ docs/                   # Documentation
```

### ğŸš€ Quick Start

#### Prerequisites

- Go 1.21+
- Python 3.8+
- Redis
- Docker (optional)

#### Installation

```bash
# Install Go dependencies
cd server
go mod download

# Install Python dependencies
pip install -r requirements.txt
```

#### Running the Server

```bash
# Start Redis
docker run -d -p 6379:6379 redis:latest

# Run Go server
cd server
go run main.go
```

#### Making Predictions

```python
from client.ml_client import MLClient

client = MLClient(base_url="http://localhost:8080")

# Single prediction
result = client.predict(
    model_name="iris_classifier",
    features=[5.1, 3.5, 1.4, 0.2]
)
print(f"Prediction: {result['prediction']}")
print(f"Probability: {result['probability']}")
```

### ğŸ“Š Performance

- **Latency**: < 1ms (p50), < 5ms (p99)
- **Throughput**: 50,000+ requests/second
- **Cache Hit Rate**: 85-95%
- **Memory Usage**: < 100MB per instance

### ğŸ“„ License

MIT License - see LICENSE file for details.

### ğŸ‘¤ Author

**Gabriel Demetrios Lafis**

---

<a name="portuguÃªs"></a>
## ğŸ‡§ğŸ‡· PortuguÃªs

### ğŸ“Š VisÃ£o Geral

**Real-Time ML Model Serving API** Ã© uma API de alta performance e pronta para produÃ§Ã£o construÃ­da com **Go** para servir modelos de machine learning em tempo real. Possui cache inteligente com Redis, monitoramento de drift de modelos, testes A/B, logging de prediÃ§Ãµes, capacidades de auto-scaling e integraÃ§Ã£o perfeita com MLflow para gerenciamento de modelos.

Este projeto demonstra as melhores prÃ¡ticas para deploy de modelos ML em produÃ§Ã£o com latÃªncia sub-milissegundo e alto throughput.

### âœ¨ Principais Recursos

- **Servidor Go de Alta Performance**
  - Tempos de resposta sub-milissegundo
  - Tratamento de requisiÃ§Ãµes concorrentes
  - Gerenciamento eficiente de memÃ³ria
  - Design de API RESTful

- **Cache Inteligente**
  - IntegraÃ§Ã£o com Redis para cache de prediÃ§Ãµes
  - EstratÃ©gias de invalidaÃ§Ã£o de cache
  - ExpiraÃ§Ã£o baseada em TTL
  - Monitoramento de taxa de acerto do cache

- **Capacidades MLOps**
  - Versionamento de modelos
  - Framework de testes A/B
  - DetecÃ§Ã£o e monitoramento de drift
  - Logging de prediÃ§Ãµes para retreinamento
  - IntegraÃ§Ã£o com MLflow

- **Pronto para ProduÃ§Ã£o**
  - ContainerizaÃ§Ã£o com Docker
  - ConfiguraÃ§Ãµes de deploy no Kubernetes
  - Health checks e mÃ©tricas
  - Shutdown gracioso
  - Suporte a auto-scaling

### ğŸ—ï¸ Arquitetura

```
realtime-ml-serving-api/
â”œâ”€â”€ server/                 # Servidor Go
â”‚   â”œâ”€â”€ main.go
â”‚   â”œâ”€â”€ handlers/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ middleware/
â”œâ”€â”€ client/                 # Cliente Python
â”‚   â”œâ”€â”€ ml_client.py
â”‚   â””â”€â”€ model_trainer.py
â”œâ”€â”€ models/                 # Modelos treinados
â”œâ”€â”€ config/                 # ConfiguraÃ§Ã£o
â”œâ”€â”€ tests/                  # Testes
â””â”€â”€ docs/                   # DocumentaÃ§Ã£o
```

### ğŸš€ InÃ­cio RÃ¡pido

#### PrÃ©-requisitos

- Go 1.21+
- Python 3.8+
- Redis
- Docker (opcional)

#### InstalaÃ§Ã£o

```bash
# Instale dependÃªncias Go
cd server
go mod download

# Instale dependÃªncias Python
pip install -r requirements.txt
```

#### Executando o Servidor

```bash
# Inicie o Redis
docker run -d -p 6379:6379 redis:latest

# Execute o servidor Go
cd server
go run main.go
```

#### Fazendo PrediÃ§Ãµes

```python
from client.ml_client import MLClient

client = MLClient(base_url="http://localhost:8080")

# PrediÃ§Ã£o Ãºnica
result = client.predict(
    model_name="iris_classifier",
    features=[5.1, 3.5, 1.4, 0.2]
)
print(f"PrediÃ§Ã£o: {result['prediction']}")
print(f"Probabilidade: {result['probability']}")
```

### ğŸ“Š Performance

- **LatÃªncia**: < 1ms (p50), < 5ms (p99)
- **Throughput**: 50.000+ requisiÃ§Ãµes/segundo
- **Taxa de Acerto do Cache**: 85-95%
- **Uso de MemÃ³ria**: < 100MB por instÃ¢ncia

### ğŸ“„ LicenÃ§a

LicenÃ§a MIT - veja o arquivo LICENSE para detalhes.

### ğŸ‘¤ Autor

**Gabriel Demetrios Lafis**

